{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2NmYZYYhXrcZ"
   },
   "source": [
    "## 환경 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6-bFpckCXrcb",
    "outputId": "041269a9-fc3e-44f9-cebd-7d26e4bd006f"
   },
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/NLP-kr/tensorflow-ml-nlp-tf2/master/requirements.txt -O requirements.txt\n",
    "!pip install -r requirements.txt\n",
    "!pip install tensorflow==2.2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cvFHjoTCXrcc"
   },
   "source": [
    "## 데이터 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HbKNloVoXrcd",
    "outputId": "7b70fd06-d1f8-48b2-b316-0c25d432261f"
   },
   "outputs": [],
   "source": [
    "!mkdir -p data_in/KOR/naver_movie\n",
    "!wget https://raw.githubusercontent.com/NLP-kr/tensorflow-ml-nlp-tf2/master/7.PRETRAIN_METHOD/data_in/KOR/naver_movie/ratings_train.txt \\\n",
    "              -O data_in/KOR/naver_movie/ratings_train.txt\n",
    "!wget https://raw.githubusercontent.com/NLP-kr/tensorflow-ml-nlp-tf2/master/7.PRETRAIN_METHOD/data_in/KOR/naver_movie/ratings_test.txt \\\n",
    "              -O data_in/KOR/naver_movie/ratings_test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xs88fDX8Xrcd",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from transformers import TFGPT2LMHeadModel\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "import gluonnlp as nlp\n",
    "from gluonnlp.data import SentencepieceTokenizer\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XgV0aK1KXrce"
   },
   "source": [
    "아레 실행 커멘드는 gpt_ckpt 폴더가 있지 않은 경우에만 실행해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XmofLC_rXrce"
   },
   "outputs": [],
   "source": [
    "import wget\n",
    "import zipfile\n",
    "\n",
    "wget.download('https://github.com/NLP-kr/tensorflow-ml-nlp-tf2/releases/download/v1.0/gpt_ckpt.zip')\n",
    "\n",
    "with zipfile.ZipFile('gpt_ckpt.zip') as z:\n",
    "    z.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TVExOYgEXrcf"
   },
   "outputs": [],
   "source": [
    "# 시각화\n",
    "\n",
    "def plot_graphs(history, string):\n",
    "    plt.plot(history.history[string])\n",
    "    plt.plot(history.history['val_'+string], '')\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(string)\n",
    "    plt.legend([string, 'val_'+string])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s6dM4ebxXrcg"
   },
   "outputs": [],
   "source": [
    "SEED_NUM = 1234\n",
    "tf.random.set_seed(SEED_NUM)\n",
    "np.random.seed(SEED_NUM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WQrjLpuV_cnI"
   },
   "source": [
    "## 프롬프트 튜닝 모델 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g8V_Qsv3_NVE"
   },
   "outputs": [],
   "source": [
    "class TFGPT2PtuningClassifier(tf.keras.Model):\n",
    "    def __init__(self, dir_path):\n",
    "        super(TFGPT2PtuningClassifier, self).__init__()\n",
    "        \n",
    "        self.gpt2 = TFGPT2LMHeadModel.from_pretrained(dir_path)\n",
    "        self.gpt2.trainable = False\n",
    "\n",
    "        self.prompt_embedding_size = self.gpt2.config.hidden_size\n",
    "        self.prompt_emgedding = tf.keras.layers.Embedding(2, self.prompt_embedding_size, name='prompt_embedding')\n",
    "        \n",
    "        self.bilstm = tf.keras.Sequential(name='prompt_bilstm')\n",
    "        self.bilstm.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(self.prompt_embedding_size, return_sequences=True)))\n",
    "        self.bilstm.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(self.prompt_embedding_size, return_sequences=True)))\n",
    "        \n",
    "        self.mlp = tf.keras.Sequential(name='prompt_mlp')\n",
    "        self.mlp.add(tf.keras.layers.Dense(self.prompt_embedding_size))\n",
    "        self.mlp.add(tf.keras.layers.ReLU())\n",
    "        self.mlp.add(tf.keras.layers.Dense(self.prompt_embedding_size))\n",
    "\n",
    "    def generate_prompt_input(self, inputs_ids):\n",
    "        inputs_embeds = self.gpt2.transformer.wte(inputs_ids[:, 1:-1])\n",
    "\n",
    "        prompt_indexs = tf.concat([inputs_ids[:, 0:1], inputs_ids[:, -1:]], axis=-1)\n",
    "        prompt_embeds = self.prompt_emgedding(prompt_indexs)\n",
    "        prompt_embeds = self.bilstm(prompt_embeds)\n",
    "        prompt_embeds = self.mlp(prompt_embeds)\n",
    "  \n",
    "        prompt_updated_inputs = tf.concat([prompt_embeds[:, 0:1, :], inputs_embeds, \n",
    "                                  prompt_embeds[:, 1:, :]],\n",
    "                                  axis=1)\n",
    "        \n",
    "        return prompt_updated_inputs\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        input_ids = inputs[0]\n",
    "        attention_mask = inputs[1] if len(inputs) > 1 else None\n",
    "\n",
    "        inputs_embeds = self.generate_prompt_input(input_ids)\n",
    "        last_hidden_states, _ = self.gpt2({'inputs_ids': None, 'inputs_embeds': inputs_embeds, 'attention_mask': attention_mask})\n",
    "        output = last_hidden_states[:, -1, :]\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCN8Lh7gXrch"
   },
   "source": [
    "## 프롬프트 튜닝을 위한 네이버 영화 리뷰 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lr76g28XA1BP"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 3\n",
    "VALID_SPLIT = 0.1\n",
    "SENT_MAX_LEN = 39"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lAaKKUqbXrch"
   },
   "outputs": [],
   "source": [
    "TOKENIZER_PATH = './gpt_ckpt/gpt2_kor_tokenizer.spiece'\n",
    "\n",
    "tokenizer = SentencepieceTokenizer(TOKENIZER_PATH)\n",
    "vocab = nlp.vocab.BERTVocab.from_sentencepiece(TOKENIZER_PATH,\n",
    "                                               mask_token=None,\n",
    "                                               sep_token='<unused0>',\n",
    "                                               cls_token=None,\n",
    "                                               unknown_token='<unk>',\n",
    "                                               padding_token='<pad>',\n",
    "                                               bos_token='<s>',\n",
    "                                               eos_token='</s>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6DlePiINXrch"
   },
   "outputs": [],
   "source": [
    "DATA_IN_PATH = './data_in/KOR'\n",
    "DATA_OUT_PATH = \"./data_out/KOR\"\n",
    "\n",
    "DATA_TRAIN_PATH = os.path.join(DATA_IN_PATH, \"naver_movie\", \"ratings_train.txt\")\n",
    "DATA_TEST_PATH = os.path.join(DATA_IN_PATH, \"naver_movie\", \"ratings_test.txt\")\n",
    "\n",
    "train_data = pd.read_csv(DATA_TRAIN_PATH, header = 0, delimiter = '\\t', quoting = 3)\n",
    "train_data = train_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4GKNnSYuXrcj"
   },
   "outputs": [],
   "source": [
    "# train_data = train_data[:50] # for test\n",
    "\n",
    "def clean_text(sent):\n",
    "    sent_clean = re.sub(\"[^가-힣ㄱ-ㅎㅏ-ㅣ\\\\s]\", \"\", sent)\n",
    "    return sent_clean\n",
    "\n",
    "def add_prompt_token(tokens):\n",
    "    return [0] + tokens + [1]\n",
    "\n",
    "train_data_sents = []\n",
    "train_attn_mask = []\n",
    "train_data_labels = []\n",
    "\n",
    "for train_sent, train_label in train_data[['document', 'label']].values:\n",
    "    train_text_label = '긍정' if train_label == 1 else '부정'\n",
    "\n",
    "    train_tokenized_text = vocab[tokenizer(clean_text(train_sent))]\n",
    "\n",
    "    tokens = [vocab[vocab.bos_token]]  \n",
    "    tokens += pad_sequences([train_tokenized_text], \n",
    "                            SENT_MAX_LEN, \n",
    "                            value=vocab[vocab.padding_token], \n",
    "                            padding='post').tolist()[0] \n",
    "    tokens = add_prompt_token(tokens)\n",
    "\n",
    "    train_attn_mask.append([1 if t != 3 else 0 for t in tokens])\n",
    "    train_data_sents.append(tokens)\n",
    "\n",
    "    label = vocab[tokenizer('긍정')] if train_label == 1 else vocab[tokenizer('부정')]\n",
    "    train_data_labels.append(label)\n",
    "\n",
    "\n",
    "train_attn_mask = np.array(train_attn_mask, dtype=np.int64)\n",
    "train_data_sents = np.array(train_data_sents, dtype=np.int64)\n",
    "train_data_labels = np.array(train_data_labels, dtype=np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c-w5GU2IxkWv",
    "outputId": "29c84da5-4cdd-47ca-e575-77bcff453233"
   },
   "outputs": [],
   "source": [
    "print('입력 토큰 인덱스: ', train_data_sents[0])\n",
    "print('어텐션 마스크: ', train_attn_mask[0])\n",
    "print('정답 라벨: ', train_data_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "12MlbiqIXrcj"
   },
   "source": [
    "## 네이버 영화 리뷰 감정 분석을 위한 프롬프트 튜닝 학습 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9J5VOzCwXrcj",
    "outputId": "c8eecfd7-6e68-4b14-f939-a5c0934ebd04"
   },
   "outputs": [],
   "source": [
    "BASE_MODEL_PATH = './gpt_ckpt'\n",
    "cls_model = TFGPT2PtuningClassifier(dir_path=BASE_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FAKyQBJ_Xrck"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "cls_model.compile(optimizer=optimizer, loss=loss, metrics=[metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YCNdkkALXrck",
    "outputId": "23f2964e-5793-4518-fc7d-f53e0c056d52"
   },
   "outputs": [],
   "source": [
    "model_name = \"tf2_gpt2_ptuning_naver_movie\"\n",
    "\n",
    "earlystop_callback = EarlyStopping(monitor='val_accuracy', min_delta=0.0001, patience=2)\n",
    "\n",
    "checkpoint_path = os.path.join(DATA_OUT_PATH, model_name, 'weights.h5')\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "if os.path.exists(checkpoint_dir):\n",
    "    print(\"{} -- Folder already exists \\n\".format(checkpoint_dir))\n",
    "else:\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    print(\"{} -- Folder create complete \\n\".format(checkpoint_dir))\n",
    "    \n",
    "cp_callback = ModelCheckpoint(\n",
    "    checkpoint_path, monitor='val_accuracy', verbose=1, save_best_only=True, save_weights_only=True)\n",
    "\n",
    "history = cls_model.fit((train_data_sents, train_attn_mask), train_data_labels, \n",
    "                        epochs=NUM_EPOCHS, \n",
    "                        batch_size=BATCH_SIZE,\n",
    "                        validation_split=VALID_SPLIT, \n",
    "                        callbacks=[earlystop_callback, cp_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "J8s2xkMcXrck",
    "outputId": "07b8d787-7bd0-46cc-e1ee-8a1ce00dea70"
   },
   "outputs": [],
   "source": [
    "plot_graphs(history, 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "sWVxJEbRXrcl",
    "outputId": "f71c7219-b11b-4bd7-bffe-c624b7736279"
   },
   "outputs": [],
   "source": [
    "plot_graphs(history, 'loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4jFe7XMeXrcl"
   },
   "source": [
    "## 네이버 영화 리뷰 모델 프롬프트 튜닝 테스트\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "za_BFNJsXrcl",
    "outputId": "16cdfa32-acd1-48be-88a6-66225338f537"
   },
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(DATA_TEST_PATH, header=0, delimiter='\\t', quoting=3)\n",
    "test_data = test_data.dropna()\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZvJV4mOcXrcl"
   },
   "outputs": [],
   "source": [
    "# test_data = test_data[:50] # for test\n",
    "\n",
    "test_data_sents = []\n",
    "test_data_labels = []\n",
    "test_attn_mask = []\n",
    "\n",
    "pred_tokens = []\n",
    "\n",
    "\n",
    "for test_sent, test_label in test_data[['document', 'label']].values:\n",
    "    test_tokenized_text = vocab[tokenizer(clean_text(test_sent))]\n",
    "\n",
    "    tokens = []\n",
    "    tokens += pad_sequences([test_tokenized_text], \n",
    "                            SENT_MAX_LEN, \n",
    "                            value=vocab[vocab.padding_token], \n",
    "                            padding='post').tolist()[0] \n",
    "    tokens = add_prompt_token(tokens)\n",
    "    test_data_sents.append(tokens)\n",
    "    mask = [1 if t != 3 else 0 for t in tokens]\n",
    "    test_attn_mask.append(mask)\n",
    "\n",
    "    label = vocab[tokenizer('긍정')] if test_label == 1 else vocab[tokenizer('부정')]\n",
    "    test_data_labels.append(label)\n",
    "    \n",
    "test_attn_mask = np.array(test_attn_mask, dtype=np.int64)\n",
    "test_data_sents = np.array(test_data_sents, dtype=np.int64)\n",
    "test_data_labels = np.array(test_data_labels, dtype=np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lrHok3-CXrcl",
    "outputId": "908bef9c-133b-4cb0-a9e1-baa7cbc221e7"
   },
   "outputs": [],
   "source": [
    "print(\"num sents, labels {}, {}\".format(len(test_data_sents), len(test_data_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hAHf4b0JXrcm",
    "outputId": "9b26654e-c5ea-439e-ed33-4508c9add548"
   },
   "outputs": [],
   "source": [
    "cls_model.load_weights(checkpoint_path)\n",
    "\n",
    "results = cls_model.evaluate((test_data_sents, test_attn_mask), test_data_labels, batch_size=1024)\n",
    "print(\"test loss, test acc: \", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ns83PcVeDGq3"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "7.4.2.gpt2_ptune_w_mask_NSMC.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
